{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S5P NO2 Tools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel 5P NO2 Tools are a compilation of Python functions to simplify management of satellite data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting s5p_no2_tools.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile s5p_no2_tools.py\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from os import listdir, rename, path, remove\n",
    "from os.path import isfile, join, getsize\n",
    "from netCDF4 import Dataset\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import calendar\n",
    "import datetime as dt\n",
    "import requests\n",
    "import re\n",
    "from socket import timeout\n",
    "import subprocess\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "Module: s5p_no2_tools.py\n",
    "============================================================================================\n",
    "Disclaimer: The code is for demonstration purposes only. Users are responsible to check for \n",
    "    accuracy and revise to fit their objective.\n",
    "\n",
    "nc_to_df adapted from read_tropomi_no2_and_dump_ascii.py by Justin Roberts-Pierel, 2015 \n",
    "    of NASA ARSET\n",
    "    Purpose of original code: To print all SDS from an TROPOMI file\n",
    "    Modified by Vikalp Mishra & Pawan Gupta, May 10 2019 to read TROPOMI data\n",
    "    Modified by Herman Tolentino, May 8, 2020 to as steps 1 and 2 of pipeline to process \n",
    "        TROPOMI NO2 data\n",
    "============================================================================================\n",
    "'''\n",
    "\n",
    "def nc_to_df(ncfile):\n",
    "    \"\"\"\n",
    "    Purpose: This converts a TROPOMI NO2 file to a Pandas DataFrame.\n",
    "\n",
    "    Notes:\n",
    "    This was adapted from read_tropomi_no2_and_dump_ascii.py by Justin Roberts-Pierel, 2015 \n",
    "    of NASA ARSET.\n",
    "    \n",
    "    Parameters:\n",
    "    ncfile: NetCD4 file from Copernicus S5P Open Data Hub\n",
    "\n",
    "    Returns:\n",
    "    dataframe: data from NetCD4 file\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        f = open(ncfile, 'r')\n",
    "    except OSError:\n",
    "        print('cannot open', ncfile)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "        \n",
    "    # read the data\n",
    "    if 'NO2___' in ncfile and 'S5P' in ncfile:\n",
    "        tic = time.perf_counter()\n",
    "        FILENAME = ncfile\n",
    "        print(ncfile+' is a TROPOMI NO2 file.')\n",
    "\n",
    "        #this is how you access the data tree in an NetCD4 file\n",
    "        SDS_NAME='nitrogendioxide_tropospheric_column'\n",
    "        file = Dataset(ncfile,'r')\n",
    "        grp='PRODUCT' \n",
    "        ds=file\n",
    "        grp='PRODUCT'\n",
    "        lat= ds.groups[grp].variables['latitude'][0][:][:]\n",
    "        lon= ds.groups[grp].variables['longitude'][0][:][:]\n",
    "        data= ds.groups[grp].variables[SDS_NAME]      \n",
    "        #get necessary attributes \n",
    "        fv=data._FillValue\n",
    "\n",
    "        #get scan time and turn it into a vector\n",
    "        scan_time= ds.groups[grp].variables['time_utc']\n",
    "        # scan_time=geolocation['Time'][:].ravel()\n",
    "\n",
    "        year = np.zeros(lat.shape)\n",
    "        mth = np.zeros(lat.shape)\n",
    "        doy = np.zeros(lat.shape)\n",
    "        hr = np.zeros(lat.shape)\n",
    "        mn = np.zeros(lat.shape)\n",
    "        sec = np.zeros(lat.shape)\n",
    "        strdatetime = np.zeros(lat.shape)\n",
    "\n",
    "        for i in range(0,lat.shape[0]):\n",
    "            t = scan_time[0][i].split('.')[0]\n",
    "            t1 = t.replace('T',' ')\n",
    "            t2 = dt.datetime.strptime(t,'%Y-%m-%dT%H:%M:%S')\n",
    "            t3 = t2.strftime(\"%s\")\n",
    "            #y = t2.year\n",
    "            #m = t2.month\n",
    "            #d = t2.day\n",
    "            #h = t2.hour\n",
    "            #m = t2.minute\n",
    "            #s = t2.second\n",
    "\n",
    "            #year[i][:] = y\n",
    "            #mth[i][:] = m\n",
    "            #doy[i][:] = d\n",
    "            #hr[i][:] = h\n",
    "            #mn[i][:] = m\n",
    "            #sec[i][:] = s\n",
    "            strdatetime[i][:] = t3\n",
    "        vlist = list(file[grp].variables.keys())\n",
    "        #df['Year'] = year.ravel()\n",
    "        #df['Month'] = mth.ravel()\n",
    "        #df['Day'] = doy.ravel()\n",
    "        #df['Hour'] = hr.ravel()\n",
    "        #df['Minute'] = mn.ravel()\n",
    "        #df['Second'] = sec.ravel()\n",
    "        df['UnixTimestamp'] = strdatetime.ravel()\n",
    "        df['DateTime'] = pd.to_datetime(df['UnixTimestamp'], unit='s')\n",
    "        df[['Date','Time']] = df['DateTime'].astype(str).str.split(' ',expand=True)\n",
    "        # This for loop saves all of the SDS in the dictionary at the top \n",
    "        #    (dependent on file type) to the array (with titles)\n",
    "        for i in range(0,len(vlist)):\n",
    "            SDS_NAME=vlist[(i)] # The name of the sds to read\n",
    "            #get current SDS data, or exit program if the SDS is not found in the file\n",
    "            #try:\n",
    "            sds=ds.groups[grp].variables[SDS_NAME]\n",
    "            if len(sds.shape) == 3:\n",
    "                print(SDS_NAME,sds.shape)\n",
    "                # get attributes for current SDS\n",
    "                if 'qa' in SDS_NAME:\n",
    "                    scale=sds.scale_factor\n",
    "                else: scale = 1.0\n",
    "                fv=sds._FillValue\n",
    "\n",
    "                # get SDS data as a vector\n",
    "                data=sds[:].ravel()\n",
    "                # The next few lines change fill value/missing value to NaN so \n",
    "                #     that we can multiply valid values by the scale factor, \n",
    "                #     then back to fill values for saving\n",
    "                data=data.astype(float)\n",
    "                data=(data)*scale\n",
    "                data[np.isnan(data)]=fv\n",
    "                data[data==float(fv)]=np.nan\n",
    "\n",
    "                df[SDS_NAME] = data\n",
    "        toc = time.perf_counter()\n",
    "        elapsed_time = toc-tic\n",
    "        print(\"Processed \"+ncfile+\" in \"+str(elapsed_time/60)+\" minutes\")\n",
    "    else:\n",
    "        raise NameError('Not a TROPOMI NO2 file name.')\n",
    "\n",
    "    return df\n",
    "\n",
    "def polygon_filter(input_df, filter_gdf):\n",
    "    \"\"\"\n",
    "    Purpose: This removes records from the TROPOMI NO2 Pandas DataFrame that\n",
    "        is not found within the filter polygons\n",
    "\n",
    "    Parameters:\n",
    "    input_df: Pandas DataFrame containing NO2 data coming from nc_to_df() \n",
    "    filter_gdf: GeoPandas GeoDataFrame containing geometries to constrain\n",
    "        NO2 records\n",
    "\n",
    "    Returns:\n",
    "    geodataframe: Filtered GeoPandas GeoDataFrame\n",
    "    \"\"\"\n",
    "    tic = time.perf_counter()\n",
    "    output_gdf = pd.DataFrame()\n",
    "    print('Processing input dataframe...')\n",
    "    crs = filter_gdf.crs\n",
    "    # 1. Convert input_df to gdf\n",
    "    gdf1 = gpd.GeoDataFrame(geometry=gpd.points_from_xy(input_df.longitude, input_df.latitude),crs=crs)\n",
    "    print('Original NO2 DataFrame length:', len(gdf1))\n",
    "    # 2. Find out intersection between African Countries GeoDataFrames (geometry) and\n",
    "    #       NO2 GeoDataFrames using Geopandas sjoin (as GeoDataFrame, gdf2)\n",
    "    sjoin_gdf = gpd.sjoin(gdf1, filter_gdf, how='inner', op='intersects')\n",
    "    # 3. Do a Pandas inner join of sjoin_gdf and df1 NO2 DataFrame (sjoin_gdf is a filter GDF)\n",
    "    #     using indexes. Inner join filters out non-intersecting records\n",
    "    gdf2 = input_df.join(sjoin_gdf, how='inner')\n",
    "    print('Filtered NO2 GeoDataFrame length:', len(gdf2))\n",
    "    toc = time.perf_counter()\n",
    "    elapsed_time = toc-tic\n",
    "    print(\"Processed NO2 DataFrame sjoin in \"+str(elapsed_time/60)+\" minutes\")\n",
    "    output_gdf = gdf2\n",
    "\n",
    "    return output_gdf\n",
    "\n",
    "def get_filename_from_cd(cd):\n",
    "    \"\"\"\n",
    "    Purpose: Get filename from content-disposition\n",
    "    \"\"\"\n",
    "    if not cd:\n",
    "        return None\n",
    "    fname = re.findall('filename=(.+)', cd)\n",
    "    if len(fname) == 0:\n",
    "        return None\n",
    "    return fname[0]\n",
    "\n",
    "def download_nc_file(url, auth, savedir, logging, refresh):\n",
    "    \"\"\"\n",
    "    Purpose: Download NetCD4 files from URL\n",
    "    \"\"\"\n",
    "    user = auth['user']\n",
    "    password = auth['password']\n",
    "    filename = 'temp.nc'\n",
    "    logfile = 'nc.log'\n",
    "    try:\n",
    "        refresh=refresh\n",
    "        headers = {\n",
    "            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 \\\n",
    "            (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "        }        \n",
    "        tic = time.perf_counter()\n",
    "        with open(savedir+'/'+filename, 'wb') as f:\n",
    "            response = requests.get(url, auth=(user, password), stream=True, headers=headers)\n",
    "            filename0 = get_filename_from_cd(response.headers.get('content-disposition')).replace('\"','')\n",
    "            if path.exists(savedir+'/'+filename0):\n",
    "                print('File '+filename0+' exists.')\n",
    "                if refresh==False:\n",
    "                    filename0_size = getsize(savedir+'/'+filename0)\n",
    "                    print('Filename size:', filename0_size,' bytes')\n",
    "                    if filename0_size > 0:\n",
    "                        remove(savedir+'/'+filename)\n",
    "                        return filename0\n",
    "            print('Downloading '+filename0+'...')\n",
    "            total = response.headers.get('content-length')\n",
    "\n",
    "            if total is None:\n",
    "                f.write(response.content)\n",
    "            else:\n",
    "                downloaded = 0\n",
    "                total = int(total)\n",
    "                for data in response.iter_content(chunk_size=max(int(total/1000), 1024*1024)):\n",
    "                    downloaded += len(data)\n",
    "                    f.write(data)\n",
    "                    done = int(50*downloaded/total)\n",
    "                    sys.stdout.write('\\r[{}{}]'.format('â–ˆ' * done, '.' * (50-done)))\n",
    "                    sys.stdout.flush()\n",
    "        if logging==True:\n",
    "            with open(logfile, 'a+') as l:\n",
    "                # Move read cursor to the start of file.\n",
    "                l.seek(0)\n",
    "                # If file is not empty then append '\\n'\n",
    "                data = l.read(100)\n",
    "                if len(data) > 0 :\n",
    "                    l.write(\"\\n\")\n",
    "                # Append text at the end of file\n",
    "                l.write(filename0)\n",
    "        sys.stdout.write('\\n')\n",
    "        rename(savedir+'/'+filename, savedir+'/'+filename0)\n",
    "        toc = time.perf_counter()\n",
    "        elapsed_time = toc-tic\n",
    "        print('Success: Saved '+filename0+' to '+savedir+'.')\n",
    "        print('Download time, seconds: '+str(elapsed_time))\n",
    "        return filename0\n",
    "    except:\n",
    "        print('Something went wrong.')\n",
    "        \n",
    "def batch_download_nc_files(auth, savedir, url_file, numfiles, logging, refresh):\n",
    "    savedir=savedir\n",
    "    url_file = url_file\n",
    "    df = pd.read_csv(url_file)\n",
    "    df['ncfile'] = ''\n",
    "    counter=0\n",
    "    numfiles = numfiles\n",
    "    if numfiles > 0:\n",
    "        print('Processing '+str(numfiles)+((' files...') if numfiles > 1 else ' file...'))\n",
    "    else:\n",
    "        print('Processing '+str(len(df))+((' files...') if len(df) > 1 else ' file...'))\n",
    "    for index, row in df.iterrows():\n",
    "        url = row['URL']\n",
    "        filename = download_nc_file(url=url, \n",
    "                                    auth=auth, \n",
    "                                    savedir='NO2-NetCD4',\n",
    "                                    logging=logging,\n",
    "                                    refresh=refresh)\n",
    "        if filename:\n",
    "            print('Downloaded file no:',counter+1)\n",
    "            print(row['URL'], filename)\n",
    "            df.loc[index,'ncfile'] = filename\n",
    "        counter += 1\n",
    "        if numfiles > 0:\n",
    "            if counter >= numfiles:\n",
    "                return df\n",
    "        delays = [7, 4, 6, 2, 10, 15, 19, 23]\n",
    "        delay = np.random.choice(delays)\n",
    "        print('Delaying for '+str(delay)+' seconds...')\n",
    "        time.sleep(delay)\n",
    "    return df\n",
    "        \n",
    "def harpconvert(input_filename, input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Purpose: This converts a TROPOMI NO2 NetCD4 file to a HDF5 (Level 3 Analysis).\n",
    "    \n",
    "    Notes: harp convert command adapted from Google Earth Engine site:\n",
    "            https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_NO2\n",
    "    \n",
    "    Parameters:\n",
    "    input_filename: NetCD4 file from Copernicus S5P Open Data Hub\n",
    "    input_dir: Directory where input_filename is located\n",
    "    output_dir: Directory where hdf5 file output is saved\n",
    "\n",
    "    Returns:\n",
    "    dictionary: NetCD4 filename, processing time in seconds, stdout, stderr\n",
    "\n",
    "    \"\"\"\n",
    "    tic = time.perf_counter()\n",
    "    input_filename = input_filename\n",
    "    output_filename = input_filename[:-3]+'.h5'\n",
    "    input_path = input_dir+'/'+input_filename\n",
    "    output_path = output_dir+'/'+output_filename\n",
    "    cmd = \"harpconvert --format hdf5 --hdf5-compression 9 \\\n",
    "        -a 'tropospheric_NO2_column_number_density_validity>50;derive(datetime_stop {time})' \\\n",
    "        %s %s\" % (input_path, output_path)\n",
    "    process = subprocess.Popen(['bash','-c', cmd],\n",
    "                     stdout=subprocess.PIPE, \n",
    "                     stderr=subprocess.PIPE)\n",
    "    filesize = getsize(output_path)\n",
    "    fs = f'{filesize:,}'\n",
    "    stdout, stderr = process.communicate()\n",
    "    toc = time.perf_counter()\n",
    "    elapsed_time = toc-tic\n",
    "    status_dict = {}\n",
    "    status_dict['input_filename'] = input_filename\n",
    "    status_dict['output_filesize'] = fs\n",
    "    status_dict['elapsed_time'] = elapsed_time\n",
    "    status_dict['stdout'] = stdout\n",
    "    status_dict['stderr'] = stderr\n",
    "    return status_dict\n",
    "\n",
    "def batch_assemble_filtered_pickles(filtered_dir):\n",
    "    tic = time.perf_counter()\n",
    "    filtered_dir = filtered_dir\n",
    "    pickle_files = [f for f in listdir(filtered_dir) if isfile(join(filtered_dir, f))]\n",
    "\n",
    "    full_df = pd.DataFrame()\n",
    "    df_len = []\n",
    "    for i in range(0, len(pickle_files)):\n",
    "        print(pickle_files[i])\n",
    "        df = pd.read_pickle('NO2-Filtered/'+pickle_files[i])\n",
    "        df_len.append(len(df))\n",
    "        full_df = pd.concat([df,full_df],axis=0)\n",
    "    toc = time.perf_counter()\n",
    "    elapsed_time = toc-tic\n",
    "    print('Assembly time, minutes: '+str(elapsed_time/60))\n",
    " \n",
    "    return full_df\n",
    "\n",
    "def plot_maps(iso3, filter_gdf, filelist, colormap, sensing_date):\n",
    "    crs = filter_gdf.crs\n",
    "    gdf_sjoin_list = []\n",
    "    country_gdf = filter_gdf[filter_gdf['iso3']==iso3]\n",
    "    country_name = list(country_gdf.loc[country_gdf['iso3']==iso3,'name'].unique())[0]\n",
    "    for file in filelist:\n",
    "        gdf_sjoin = pd.read_pickle('./'+file).set_geometry('geometry')\n",
    "        gdf_sjoin.crs = crs\n",
    "        gdf_sjoin.drop(columns=['index_right'], inplace=True)\n",
    "        gdf_countries_sjoin = gpd.sjoin(gdf_sjoin, country_gdf, how='inner', op='intersects')\n",
    "        if len(gdf_countries_sjoin) > 0:\n",
    "            gdf_sjoin_list.append(gdf_countries_sjoin)\n",
    "    swaths = len(gdf_sjoin_list)\n",
    "    print('Using '+str(swaths)+' swaths.')\n",
    "    \n",
    "    qa_vmax = []\n",
    "    qa_vmin = []\n",
    "    column='qa_value'\n",
    "    for gdf in gdf_sjoin_list:\n",
    "        qa_vmax.append(gdf[column].max())\n",
    "        qa_vmin.append(gdf[column].min())\n",
    "    vmax_qa = max(qa_vmax)\n",
    "    vmin_qa = min(qa_vmin)\n",
    "    \n",
    "    no2_vmax = []\n",
    "    no2_vmin = []\n",
    "    column='nitrogendioxide_tropospheric_column'\n",
    "    for gdf in gdf_sjoin_list:\n",
    "        no2_vmax.append(gdf[column].max())\n",
    "        no2_vmin.append(gdf[column].min())\n",
    "    vmax_no2 = max(no2_vmax)\n",
    "    vmin_no2 = min(no2_vmin)\n",
    "\n",
    "    colormap=colormap\n",
    "    fig, ax= plt.subplots(sharex=True, sharey=True, figsize=(8,6), constrained_layout=True)\n",
    "    for gdf in gdf_sjoin_list:\n",
    "        gdf.plot(cmap=plt.get_cmap(colormap), ax=ax,\n",
    "                 column='qa_value', vmin=vmin_qa, vmax=vmax_qa, alpha=0.9)\n",
    "    country_gdf.plot(ax=ax, alpha=0.1, color='None')\n",
    "\n",
    "    # add colorbar\n",
    "    fig = ax.get_figure()\n",
    "    #cax = fig.add_axes([0.9, 0.2, 0.03, 0.6])\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"3%\", pad=0.1)\n",
    "    sm = plt.cm.ScalarMappable(cmap=colormap, norm=plt.Normalize(vmin=vmin_qa, vmax=vmax_qa))\n",
    "    sm._A = []\n",
    "    fig.colorbar(sm, cax=cax)\n",
    "    plt.suptitle('Tropospheric NO2, QA Value ('+country_name+', '+sensing_date+')')\n",
    "\n",
    "    fig, ax= plt.subplots(sharex=True, sharey=True, figsize=(8,6), constrained_layout=True)\n",
    "    for gdf in gdf_sjoin_list:\n",
    "        gdf.plot(cmap=plt.get_cmap(colormap), ax=ax,\n",
    "                 column='nitrogendioxide_tropospheric_column_precision_kernel', \n",
    "                 vmin=vmin_no2, vmax=vmax_no2, alpha=0.9)\n",
    "    country_gdf.plot(ax=ax, alpha=0.1, color='None',legend=False)\n",
    "\n",
    "    # add colorbar\n",
    "    fig = ax.get_figure()\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"3%\", pad=0.1)\n",
    "    sm = plt.cm.ScalarMappable(cmap=colormap, norm=plt.Normalize(vmin=vmin_no2, vmax=vmax_no2))\n",
    "    sm._A = []\n",
    "    fig.colorbar(sm, cax=cax)\n",
    "    plt.suptitle('Tropospheric NO2, Tropospheric Column, moles/m2 ('+country_name+', '+sensing_date+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
