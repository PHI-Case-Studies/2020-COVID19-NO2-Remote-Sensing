{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S5P NO2 Tools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel 5P NO2 Tools are a compilation of Python functions to simplify management of satellite data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting s5p_no2_tools.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile s5p_no2_tools.py\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from os import listdir, rename, path, remove, mkdir\n",
    "from os.path import isfile, join, getsize, exists\n",
    "from netCDF4 import Dataset\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import calendar\n",
    "import datetime as dt\n",
    "import re\n",
    "from socket import timeout\n",
    "import subprocess\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import requests, json\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "\n",
    "'''\n",
    "Module: s5p_no2_tools.py\n",
    "============================================================================================\n",
    "Disclaimer: The code is for demonstration purposes only. Users are responsible to check for \n",
    "    accuracy and revise to fit their objective.\n",
    "\n",
    "nc_to_df adapted from read_tropomi_no2_and_dump_ascii.py by Justin Roberts-Pierel, 2015 \n",
    "    of NASA ARSET\n",
    "    Purpose of original code: To print all SDS from an TROPOMI file\n",
    "    Modified by Vikalp Mishra & Pawan Gupta, May 10 2019 to read TROPOMI data\n",
    "    Modified by Herman Tolentino, May 8, 2020 to as steps 1 and 2 of pipeline to process \n",
    "        TROPOMI NO2 data\n",
    "============================================================================================\n",
    "'''\n",
    "def create_project(project_name='default'):\n",
    "    \"\"\"\n",
    "    Purpose: Create project subfolder\n",
    "    Parameters:\n",
    "        project_name (string): subfolder to create\n",
    "    Returns:\n",
    "        project_name (string)\n",
    "    \"\"\"\n",
    "    if not exists(project_name):\n",
    "        try:\n",
    "            mkdir(project_name)\n",
    "        except OSError:\n",
    "            print (f\"Creation of the directory {project_name} failed.\")\n",
    "            return ''\n",
    "        else:\n",
    "            print (f\"Successfully created the directory {project_name}.\")\n",
    "            return project_name\n",
    "    else:\n",
    "        print(f\"Directory {project_name} exists.\")\n",
    "        return project_name\n",
    "    \n",
    "def get_place_boundingbox(place_gdf, buffer):\n",
    "    \"\"\"\n",
    "    Purpose: Determine the bounding box of a place GeoDataFrame (polygon)\n",
    "    Parameters:\n",
    "        place_gdf (GeoDataFrame): place GeoDataFrame to get bounding box for. This should be\n",
    "            a Level 0 polygon from GADM.\n",
    "        buffer (int): buffer in miles to add to place geometry\n",
    "    Returns:\n",
    "        bbox (GeoDataFrame): GeoDataFrame containing the bounding box as geometry\n",
    "    \"\"\"\n",
    "    bbox = gpd.GeoDataFrame(geometry=place_gdf['geometry'].buffer(buffer).envelope, crs=place_gdf.crs)\\\n",
    "    .reset_index()\n",
    "\n",
    "    return bbox\n",
    "    \n",
    "def filter_swath_set(swath_set_gdf, place_gdf):\n",
    "    \"\"\"\n",
    "    Purpose: Reduce the number of swaths based on place constraint.\n",
    "    Parameters:\n",
    "        swath_set_gdf (GeoDataFrame): A GeoDataFrame output from sending a query to the \n",
    "                                      sentinel_api_query() function. This GeoDataFrame contains\n",
    "                                      a geometry for the swath. This geometry should contain\n",
    "                                      the place_gdf geometry below.\n",
    "        place_gdf (GeoDataFrame):     A GeoDataFrame that should be contained within the\n",
    "                                      swath_set_gdf geometry.\n",
    "    Returns:\n",
    "        filtered_gdf (GeoDataFrame): A subset of swath_set_gdf representing geometries that\n",
    "                                     contain the place_gdf geometry.\n",
    "    \"\"\"\n",
    "    filtered_gdf = gpd.sjoin(swath_set_gdf, place_gdf, how='right', op='contains').reset_index()\n",
    "    filtered_gdf = filtered_gdf.drop(columns=['level_0','index_left','index'])\n",
    "    return filtered_gdf\n",
    "\n",
    "def geometry_to_wkt(place_gdf):\n",
    "    \"\"\"\n",
    "    Purpose: Sentinel 5P Data Access hub requires a constraining polygon filter to \n",
    "             retrieve a smaller number of satellite image swaths.\n",
    "    \n",
    "    Parameters:\n",
    "        place_gdf (GeoDataFrame): Target place for obtaining NO2 levels. Place GDF should be\n",
    "                                  a simple, GADM level 0 polygon.\n",
    "\n",
    "    Returns:\n",
    "        wkt_string (string): string containing polygon vertices in WKT format\n",
    "    \"\"\"\n",
    "    # get geometry convex hull and simplify\n",
    "    geometry = place_gdf.reset_index()['geometry'].convex_hull.simplify(tolerance=0.05)\n",
    "    \n",
    "    # convert to WKT\n",
    "    wkt_string = wkt.dumps(geometry[0])\n",
    "    \n",
    "    return wkt_string\n",
    "\n",
    "def date_from_week(weekstring='2019-W01'):\n",
    "    d = weekstring\n",
    "    r = dt.datetime.strptime(d + '-1', \"%Y-W%W-%w\")\n",
    "    return r\n",
    "\n",
    "def add_days(start, numDays=1):\n",
    "    end = start + dt.timedelta(days=numDays)\n",
    "    startDate = start.strftime(\"%Y-%m-%d\")\n",
    "    endDate = end.strftime(\"%Y-%m-%d\")\n",
    "    return [startDate, endDate]\n",
    "\n",
    "def nc_to_df(ncfile):\n",
    "    \"\"\"\n",
    "    Purpose: This converts a TROPOMI NO2 file to a Pandas DataFrame.\n",
    "\n",
    "    Notes:\n",
    "    This was adapted from read_tropomi_no2_and_dump_ascii.py by Justin Roberts-Pierel, 2015 \n",
    "    of NASA ARSET.\n",
    "    \n",
    "    Parameters:\n",
    "    ncfile: NetCD4 file from Copernicus S5P Open Data Hub\n",
    "\n",
    "    Returns:\n",
    "    dataframe: data from NetCD4 file\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        f = open(ncfile, 'r')\n",
    "    except OSError:\n",
    "        print('cannot open', ncfile)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "        \n",
    "    # read the data\n",
    "    if 'NO2___' in ncfile and 'S5P' in ncfile:\n",
    "        tic = time.perf_counter()\n",
    "        FILENAME = ncfile\n",
    "        print(ncfile+' is a TROPOMI NO2 file.')\n",
    "\n",
    "        #this is how you access the data tree in an NetCD4 file\n",
    "        SDS_NAME='nitrogendioxide_tropospheric_column'\n",
    "        file = Dataset(ncfile,'r')\n",
    "        grp='PRODUCT' \n",
    "        ds=file\n",
    "        grp='PRODUCT'\n",
    "        lat= ds.groups[grp].variables['latitude'][0][:][:]\n",
    "        lon= ds.groups[grp].variables['longitude'][0][:][:]\n",
    "        data= ds.groups[grp].variables[SDS_NAME]      \n",
    "        #get necessary attributes \n",
    "        fv=data._FillValue\n",
    "\n",
    "        #get scan time and turn it into a vector\n",
    "        scan_time= ds.groups[grp].variables['time_utc']\n",
    "        # scan_time=geolocation['Time'][:].ravel()\n",
    "\n",
    "        year = np.zeros(lat.shape)\n",
    "        mth = np.zeros(lat.shape)\n",
    "        doy = np.zeros(lat.shape)\n",
    "        hr = np.zeros(lat.shape)\n",
    "        mn = np.zeros(lat.shape)\n",
    "        sec = np.zeros(lat.shape)\n",
    "        strdatetime = np.zeros(lat.shape)\n",
    "\n",
    "        for i in range(0,lat.shape[0]):\n",
    "            t = scan_time[0][i].split('.')[0]\n",
    "            t1 = t.replace('T',' ')\n",
    "            t2 = dt.datetime.strptime(t,'%Y-%m-%dT%H:%M:%S')\n",
    "            t3 = t2.strftime(\"%s\")\n",
    "            #y = t2.year\n",
    "            #m = t2.month\n",
    "            #d = t2.day\n",
    "            #h = t2.hour\n",
    "            #m = t2.minute\n",
    "            #s = t2.second\n",
    "\n",
    "            #year[i][:] = y\n",
    "            #mth[i][:] = m\n",
    "            #doy[i][:] = d\n",
    "            #hr[i][:] = h\n",
    "            #mn[i][:] = m\n",
    "            #sec[i][:] = s\n",
    "            strdatetime[i][:] = t3\n",
    "        vlist = list(file[grp].variables.keys())\n",
    "        #df['Year'] = year.ravel()\n",
    "        #df['Month'] = mth.ravel()\n",
    "        #df['Day'] = doy.ravel()\n",
    "        #df['Hour'] = hr.ravel()\n",
    "        #df['Minute'] = mn.ravel()\n",
    "        #df['Second'] = sec.ravel()\n",
    "        df['UnixTimestamp'] = strdatetime.ravel()\n",
    "        df['DateTime'] = pd.to_datetime(df['UnixTimestamp'], unit='s')\n",
    "        df[['Date','Time']] = df['DateTime'].astype(str).str.split(' ',expand=True)\n",
    "        # This for loop saves all of the SDS in the dictionary at the top \n",
    "        #    (dependent on file type) to the array (with titles)\n",
    "        for i in range(0,len(vlist)):\n",
    "            SDS_NAME=vlist[(i)] # The name of the sds to read\n",
    "            #get current SDS data, or exit program if the SDS is not found in the file\n",
    "            #try:\n",
    "            sds=ds.groups[grp].variables[SDS_NAME]\n",
    "            if len(sds.shape) == 3:\n",
    "                print(SDS_NAME,sds.shape)\n",
    "                # get attributes for current SDS\n",
    "                if 'qa' in SDS_NAME:\n",
    "                    scale=sds.scale_factor\n",
    "                else: scale = 1.0\n",
    "                fv=sds._FillValue\n",
    "\n",
    "                # get SDS data as a vector\n",
    "                data=sds[:].ravel()\n",
    "                # The next few lines change fill value/missing value to NaN so \n",
    "                #     that we can multiply valid values by the scale factor, \n",
    "                #     then back to fill values for saving\n",
    "                data=data.astype(float)\n",
    "                data=(data)*scale\n",
    "                data[np.isnan(data)]=fv\n",
    "                data[data==float(fv)]=np.nan\n",
    "\n",
    "                df[SDS_NAME] = data\n",
    "        toc = time.perf_counter()\n",
    "        elapsed_time = toc-tic\n",
    "        print(\"Processed \"+ncfile+\" in \"+str(elapsed_time/60)+\" minutes\")\n",
    "    else:\n",
    "        raise NameError('Not a TROPOMI NO2 file name.')\n",
    "\n",
    "    return df\n",
    "\n",
    "def polygon_filter(input_df, filter_gdf):\n",
    "    \"\"\"\n",
    "    Purpose: This removes records from the TROPOMI NO2 Pandas DataFrame that\n",
    "        is not found within the filter polygons\n",
    "\n",
    "    Parameters:\n",
    "    input_df: Pandas DataFrame containing NO2 data coming from nc_to_df() \n",
    "    filter_gdf: GeoPandas GeoDataFrame containing geometries to constrain\n",
    "        NO2 records. Be sure to create the spatial index for filter_gdf to\n",
    "        speed up sjoin operation. You can do this by calling\n",
    "        filter_gdf.sindex before feeding filter_gdf into:\n",
    "        polygon_filter(input_df=input_df, filter_gdf=filter_gdf)\n",
    "\n",
    "    Returns:\n",
    "    geodataframe: Filtered GeoPandas GeoDataFrame\n",
    "    \"\"\"\n",
    "    print('To speed up the polygon_filter() operation, did you create the spatial index for filter_gdf?')\n",
    "    tic = time.perf_counter()\n",
    "    output_gdf = pd.DataFrame()\n",
    "    print('Processing input dataframe...')\n",
    "    crs = filter_gdf.crs\n",
    "    # 1. Convert input_df to gdf\n",
    "    gdf1 = gpd.GeoDataFrame(input_df, geometry=gpd.points_from_xy(input_df.longitude, input_df.latitude),crs=crs)\n",
    "    print('Original NO2 DataFrame length:', len(gdf1))\n",
    "    # 2. Find out intersection between African Countries GeoDataFrames (geometry) and\n",
    "    #       NO2 GeoDataFrames using Geopandas sjoin (as GeoDataFrame, gdf2)\n",
    "    sjoin_gdf = gpd.sjoin(gdf1, filter_gdf, how='inner', op='intersects')\n",
    "    print('Filtered NO2 GeoDataFrame length:', len(sjoin_gdf))\n",
    "    toc = time.perf_counter()\n",
    "    elapsed_time = toc-tic\n",
    "    print(\"Processed NO2 DataFrame sjoin in \"+str(elapsed_time/60)+\" minutes\")\n",
    "\n",
    "    return sjoin_gdf\n",
    "\n",
    "def get_filename_from_cd(cd):\n",
    "    \"\"\"\n",
    "    Purpose: Get filename from content-disposition (cd)\n",
    "    \n",
    "    Parameters:\n",
    "        cd (string): content-disposition\n",
    "\n",
    "    Returns:\n",
    "        fname[0] (string): filename\n",
    "    \"\"\"\n",
    "    if not cd:\n",
    "        return None\n",
    "    fname = re.findall('filename=(.+)', cd)\n",
    "    if len(fname) == 0:\n",
    "        return None\n",
    "    return fname[0]\n",
    "\n",
    "def download_nc_file(url, auth, savedir, logging, refresh):\n",
    "    \"\"\"\n",
    "    Purpose: Download NetCD4 files from URL\n",
    "    \n",
    "    Parameters:\n",
    "    url: string, download url obtained from Sentinel 5P Open Data Hub search results\n",
    "    auth: dictionary of 'user' and 'password'\n",
    "    savedir: string, path to save NetCD4 files\n",
    "    logging: boolean, turn logging on or off\n",
    "    refresh: boolean, overwrite previously downloaded files (helps save time if False)\n",
    "\n",
    "    Returns:\n",
    "    filename: string filename of NetCD4 file\n",
    "    \"\"\"\n",
    "    user = auth['user']\n",
    "    password = auth['password']\n",
    "    filename = 'temp.nc'\n",
    "    logfile = 'nc.log'\n",
    "    try:\n",
    "        refresh=refresh\n",
    "        headers = {\n",
    "            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 \\\n",
    "            (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "        }\n",
    "        tic = time.perf_counter()\n",
    "        with open(savedir+'/'+filename, 'wb') as f:\n",
    "            response = requests.get(url, auth=(user, password), stream=True, headers=headers)\n",
    "            filename0 = get_filename_from_cd(response.headers.get('content-disposition')).replace('\"','')\n",
    "            if path.exists(savedir+'/'+filename0):\n",
    "                print('File '+filename0+' exists.')\n",
    "                if refresh==False:\n",
    "                    filename0_size = getsize(savedir+'/'+filename0)\n",
    "                    print('Filename size:', filename0_size,' bytes')\n",
    "                    if filename0_size > 0:\n",
    "                        remove(savedir+'/'+filename)\n",
    "                        return filename0\n",
    "            print('Downloading '+filename0+'...')\n",
    "            total = response.headers.get('content-length')\n",
    "\n",
    "            if total is None:\n",
    "                f.write(response.content)\n",
    "            else:\n",
    "                downloaded = 0\n",
    "                total = int(total)\n",
    "                for data in response.iter_content(chunk_size=max(int(total/1000), 1024*1024)):\n",
    "                    downloaded += len(data)\n",
    "                    f.write(data)\n",
    "                    done = int(50*downloaded/total)\n",
    "                    sys.stdout.write('\\r[{}{}]'.format('â–ˆ' * done, '.' * (50-done)))\n",
    "                    sys.stdout.flush()\n",
    "        if logging==True:\n",
    "            with open(logfile, 'a+') as l:\n",
    "                # Move read cursor to the start of file.\n",
    "                l.seek(0)\n",
    "                # If file is not empty then append '\\n'\n",
    "                data = l.read(100)\n",
    "                if len(data) > 0 :\n",
    "                    l.write(\"\\n\")\n",
    "                # Append text at the end of file\n",
    "                l.write(filename0)\n",
    "        sys.stdout.write('\\n')\n",
    "        rename(savedir+'/'+filename, savedir+'/'+filename0)\n",
    "        toc = time.perf_counter()\n",
    "        elapsed_time = toc-tic\n",
    "        print('Success: Saved '+filename0+' to '+savedir+'.')\n",
    "        print('Download time, seconds: '+str(elapsed_time))\n",
    "        delays = [7, 4, 6, 2, 10, 15, 19, 23]\n",
    "        delay = np.random.choice(delays)\n",
    "        print('Delaying for '+str(delay)+' seconds...')\n",
    "        time.sleep(delay)\n",
    "        return filename0\n",
    "    except:\n",
    "        print('Something went wrong.')\n",
    "        \n",
    "def batch_download_nc_files(auth, savedir, url_file, numfiles, logging, refresh):\n",
    "    \"\"\"\n",
    "    Purpose: For batch downloading nc files from the Copernicus S5P Data Access Hub\n",
    "    \n",
    "    Parameters:\n",
    "        auth (dict): authentication dictionary, {'user':'myusername', 'password':'mypassword'}\n",
    "        savedir (string): directory used to save NetCD4  files\n",
    "        url_file (string): file containing NetCD4 download URLS\n",
    "        numfiles (int)\n",
    "        logging (bool)\n",
    "        refresh (bool)\n",
    "    Returns:\n",
    "        df (DataFrame)\n",
    "    \"\"\"\n",
    "    savedir=savedir\n",
    "    url_file = url_file\n",
    "    df = pd.read_csv(url_file)\n",
    "    df['ncfile'] = ''\n",
    "    counter=0\n",
    "    numfiles = numfiles\n",
    "    if numfiles > 0:\n",
    "        print('Processing '+str(numfiles)+((' files...') if numfiles > 1 else ' file...'))\n",
    "    else:\n",
    "        print('Processing '+str(len(df))+((' files...') if len(df) > 1 else ' file...'))\n",
    "    for index, row in df.iterrows():\n",
    "        url = row['URL']\n",
    "        filename = download_nc_file(url=url, \n",
    "                                    auth=auth, \n",
    "                                    savedir='NO2-NetCD4',\n",
    "                                    logging=logging,\n",
    "                                    refresh=refresh)\n",
    "        if filename:\n",
    "            print('Downloaded file no:',counter+1)\n",
    "            print(row['URL'], filename)\n",
    "            df.loc[index,'ncfile'] = filename\n",
    "        counter += 1\n",
    "        if numfiles > 0:\n",
    "            if counter >= numfiles:\n",
    "                return df\n",
    "        delays = [7, 4, 6, 2, 10, 15, 19, 23]\n",
    "        delay = np.random.choice(delays)\n",
    "        print('Delaying for '+str(delay)+' seconds...')\n",
    "        time.sleep(delay)\n",
    "    return df\n",
    "        \n",
    "def harpconvert(input_filename, input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Purpose: This converts a TROPOMI NO2 NetCD4 file to a HDF5 (Level 3 Analysis).\n",
    "    \n",
    "    Notes: harp convert command adapted from Google Earth Engine site:\n",
    "            https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_NO2\n",
    "           This assumes harp has been installed using: conda install -c stcorp 'harp'.\n",
    "    \n",
    "    Parameters:\n",
    "    input_filename: NetCD4 file from Copernicus S5P Open Data Hub\n",
    "    input_dir: Directory where input_filename is located\n",
    "    output_dir: Directory where hdf5 file output is saved\n",
    "\n",
    "    Returns:\n",
    "    dictionary: NetCD4 filename, processing time in seconds, stdout, stderr\n",
    "\n",
    "    \"\"\"\n",
    "    tic = time.perf_counter()\n",
    "    input_filename = input_filename\n",
    "    output_filename = input_filename[:-3]+'.h5'\n",
    "    input_path = input_dir+'/'+input_filename\n",
    "    output_path = output_dir+'/'+output_filename\n",
    "    cmd = \"harpconvert --format hdf5 --hdf5-compression 9 \\\n",
    "        -a 'tropospheric_NO2_column_number_density_validity>50;derive(datetime_stop {time})' \\\n",
    "        %s %s\" % (input_path, output_path)\n",
    "    process = subprocess.Popen(['bash','-c', cmd],\n",
    "                     stdout=subprocess.PIPE, \n",
    "                     stderr=subprocess.PIPE)\n",
    "    filesize = getsize(output_path)\n",
    "    fs = f'{filesize:,}'\n",
    "    stdout, stderr = process.communicate()\n",
    "    toc = time.perf_counter()\n",
    "    elapsed_time = toc-tic\n",
    "    status_dict = {}\n",
    "    status_dict['input_filename'] = input_filename\n",
    "    status_dict['output_filesize'] = fs\n",
    "    status_dict['elapsed_time'] = elapsed_time\n",
    "    status_dict['stdout'] = stdout\n",
    "    status_dict['stderr'] = stderr\n",
    "    return status_dict\n",
    "\n",
    "def batch_assemble_filtered_pickles(filtered_dir):\n",
    "    tic = time.perf_counter()\n",
    "    filtered_dir = filtered_dir\n",
    "    pickle_files = [f for f in listdir(filtered_dir) if isfile(join(filtered_dir, f))]\n",
    "\n",
    "    full_df = pd.DataFrame()\n",
    "    df_len = []\n",
    "    for i in range(0, len(pickle_files)):\n",
    "        print(pickle_files[i])\n",
    "        df = pd.read_pickle('NO2-Filtered/'+pickle_files[i])\n",
    "        df_len.append(len(df))\n",
    "        full_df = pd.concat([df,full_df],axis=0)\n",
    "    toc = time.perf_counter()\n",
    "    elapsed_time = toc-tic\n",
    "    print('Assembly time, minutes: '+str(elapsed_time/60))\n",
    " \n",
    "    return full_df\n",
    "\n",
    "def plot_maps(iso3, filter_gdf, filelist, colormap, sensing_date):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        iso3 (string): 3-letter ISO code for country\n",
    "        filter_gdf (GeoDataFrame): \n",
    "        filelist (list): \n",
    "        colormap (string): colormap for colorbar\n",
    "        sensing_date (string):\n",
    "    Returns:\n",
    "        Matplotlib Plot\n",
    "    \"\"\"\n",
    "    crs = filter_gdf.crs\n",
    "    gdf_sjoin_list = []\n",
    "    country_gdf = filter_gdf[filter_gdf['iso3']==iso3]\n",
    "    country_name = list(country_gdf.loc[country_gdf['iso3']==iso3,'name'].unique())[0]\n",
    "    for file in filelist:\n",
    "        gdf_sjoin = pd.read_pickle(file).set_geometry('geometry')\n",
    "        gdf_sjoin.crs = crs\n",
    "        gdf_sjoin.drop(columns=['index_right'], inplace=True)\n",
    "        gdf_countries_sjoin = gpd.sjoin(gdf_sjoin, country_gdf, how='inner', op='intersects')\n",
    "        if len(gdf_countries_sjoin) > 0:\n",
    "            gdf_sjoin_list.append(gdf_countries_sjoin)\n",
    "    swaths = len(gdf_sjoin_list)\n",
    "    print('Using '+str(swaths)+' swaths.')\n",
    "    \n",
    "    qa_vmax = []\n",
    "    qa_vmin = []\n",
    "    column='qa_value'\n",
    "    for gdf in gdf_sjoin_list:\n",
    "        qa_vmax.append(gdf[column].max())\n",
    "        qa_vmin.append(gdf[column].min())\n",
    "    vmax_qa = max(qa_vmax)\n",
    "    vmin_qa = min(qa_vmin)\n",
    "    \n",
    "    no2_vmax = []\n",
    "    no2_vmin = []\n",
    "    column='nitrogendioxide_tropospheric_column'\n",
    "    for gdf in gdf_sjoin_list:\n",
    "        no2_vmax.append(gdf[column].max())\n",
    "        no2_vmin.append(gdf[column].min())\n",
    "    vmax_no2 = max(no2_vmax)\n",
    "    vmin_no2 = min(no2_vmin)\n",
    "\n",
    "    colormap=colormap\n",
    "    fig, ax= plt.subplots(sharex=True, sharey=True, figsize=(8,6), constrained_layout=True)\n",
    "    for gdf in gdf_sjoin_list:\n",
    "        gdf.plot(cmap=plt.get_cmap(colormap), ax=ax,\n",
    "                 column='qa_value', vmin=vmin_qa, vmax=vmax_qa, alpha=0.9)\n",
    "    country_gdf.plot(ax=ax, alpha=0.1, color='None')\n",
    "\n",
    "    # add colorbar\n",
    "    fig = ax.get_figure()\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"3%\", pad=0.1)\n",
    "    sm = plt.cm.ScalarMappable(cmap=colormap, norm=plt.Normalize(vmin=vmin_qa, vmax=vmax_qa))\n",
    "    sm._A = []\n",
    "    fig.colorbar(sm, cax=cax)\n",
    "    plt.suptitle('Tropospheric NO2, QA Value ('+country_name+', '+sensing_date+')')\n",
    "\n",
    "    fig, ax= plt.subplots(sharex=True, sharey=True, figsize=(8,6), constrained_layout=True)\n",
    "    for gdf in gdf_sjoin_list:\n",
    "        gdf.plot(cmap=plt.get_cmap(colormap), ax=ax,\n",
    "                 column='nitrogendioxide_tropospheric_column_precision_kernel', \n",
    "                 vmin=vmin_no2, vmax=vmax_no2, alpha=0.9)\n",
    "    country_gdf.plot(ax=ax, alpha=0.1, color='None',legend=False)\n",
    "\n",
    "    # add colorbar\n",
    "    fig = ax.get_figure()\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"3%\", pad=0.1)\n",
    "    sm = plt.cm.ScalarMappable(cmap=colormap, norm=plt.Normalize(vmin=vmin_no2, vmax=vmax_no2))\n",
    "    sm._A = []\n",
    "    fig.colorbar(sm, cax=cax)\n",
    "    plt.suptitle('Tropospheric NO2, Tropospheric Column, moles/m2 ('+country_name+', '+sensing_date+')')\n",
    "    \n",
    "\n",
    "def sentinel_api_query(query_dict,silentmode=False):\n",
    "    \"\"\"\n",
    "    Author: Herman Tolentino MD\n",
    "    Parameters:\n",
    "        query_dict: dictionary of API query variables (see below)\n",
    "    query_dict = {\n",
    "        'startDate': startDate,             # YYYY-MM-DD\n",
    "        'endDate': endDate,                 # YYYY-MM-DD\n",
    "        'productType': productType,         # L2__NO2___\n",
    "        'processingLevel': processingLevel, # L2\n",
    "        'platformName': platformName,       # Sentinel-5\n",
    "        'processingMode': processingMode,   # Offline\n",
    "        'polygon': polygon,                 # Shapefile in WKT format\n",
    "        'startPage': startPage,             # 0\n",
    "        'numRows': numRows,                 # 50-100 (usual number for continent level)\n",
    "        'dhus_url': dhus_url,               # Data Access Hub URL for S5P\n",
    "        'username': username,               # s5pguest\n",
    "        'password': password                # s5pguest\n",
    "    }\n",
    "    \"\"\"\n",
    "    delays = [7, 4, 6, 2, 10, 15, 19, 23]\n",
    "    delay = np.random.choice(delays)\n",
    "    print('Delaying for '+str(delay)+' seconds...') if silentmode==False else None\n",
    "    time.sleep(delay)\n",
    "    # STEP 1: Unpack query_dict and check data (TODO)\n",
    "    polygon = query_dict['polygon']\n",
    "    startDate = query_dict['startDate']\n",
    "    endDate = query_dict['endDate']\n",
    "    platformName = query_dict['platformName']\n",
    "    productType = query_dict['productType']\n",
    "    processingLevel = query_dict['processingLevel']\n",
    "    processingMode = query_dict['processingMode']\n",
    "    dhus_url = query_dict['dhus_url']\n",
    "    startPage = query_dict['startPage']\n",
    "    numRows = query_dict['numRows']\n",
    "    username = query_dict['username']\n",
    "    password = query_dict['password']\n",
    "    # check data here before proceeding (TODO)\n",
    "    \n",
    "    # STEP 2: Construct query string for API\n",
    "    query=f'( footprint:\"Intersects({polygon})\") AND \\\n",
    "        ( beginPosition:[{startDate}T00:00:00.000Z TO {endDate}T23:59:59.999Z] AND \\\n",
    "        endPosition:[{startDate}T00:00:00.000Z TO {endDate}T23:59:59.999Z] ) \\\n",
    "        AND ( (platformname:{platformName} AND producttype:{productType} \\\n",
    "        AND processinglevel:{processingLevel} AND processingmode:{processingMode}))'.replace('  ',' ')\n",
    "    print('query:', query) if silentmode==False else None\n",
    "    \n",
    "    quoted = urllib.parse.quote_plus(query)\n",
    "    print('quoted:', quoted) if silentmode==False else None\n",
    "    \n",
    "    # STEP 3: Send query to API and get convert XML response to dictionary\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    response = requests.get(f'{dhus_url}dhus/search?q={quoted}&start={startPage}&rows={numRows}', \\\n",
    "                     auth=HTTPBasicAuth(username, password),\\\n",
    "                    headers=headers)\n",
    "    print('headers:', response.headers) if silentmode==False else None\n",
    "    print('cookie:', response.headers['Set-Cookie']) if silentmode==False else None\n",
    "    # Convert XML response to dictionary\n",
    "    my_dict = xmltodict.parse(response.text)\n",
    "    results = int(my_dict['feed']['opensearch:totalResults'])\n",
    "    if results > numRows:\n",
    "        print('==========================================================================================')\n",
    "        print('WARNING: Returned results '+str(results)+' exceeds requested number of rows ('+str(numRows)+'). ')\n",
    "        print('==========================================================================================')\n",
    "    itemsperpage = int(my_dict['feed']['opensearch:itemsPerPage'])\n",
    "    print('search term:', my_dict['feed']['opensearch:Query']['@searchTerms']) if silentmode==False else None\n",
    "\n",
    "    # STEP 4: Store dictionary items in Pandas DataFrame\n",
    "    record_dict = {}\n",
    "    study_df = pd.DataFrame()\n",
    "    for item in my_dict['feed']['entry']:\n",
    "        print(item)\n",
    "        #print(item['title'])\n",
    "        #print('summary:', item['summary'])\n",
    "        #print('date:', item['date'][0])\n",
    "        print('=========') if silentmode==False else None\n",
    "        gmldict = xmltodict.parse(item['str'][1]['#text'])\n",
    "        crs = gmldict['gml:Polygon']['@srsName'].split('#')\n",
    "        record_dict['ingestiondate'] = item['date'][0]['#text']\n",
    "        record_dict['beginposition'] = item['date'][1]['#text']\n",
    "        record_dict['endposition'] = item['date'][2]['#text']\n",
    "        record_dict['orbitnumber'] = item['int']['#text']\n",
    "        record_dict['filename'] = item['str'][0]['#text']\n",
    "        record_dict['crs'] = 'epsg:'+crs[1]\n",
    "        record_dict['format'] = item['str'][2]['#text']\n",
    "        record_dict['identifier'] = item['str'][3]['#text']\n",
    "        record_dict['instrumentname'] = item['str'][4]['#text']\n",
    "        record_dict['instrumentshortname'] = item['str'][5]['#text']\n",
    "        record_dict['footprint'] = item['str'][6]['#text']\n",
    "        record_dict['mission'] = item['str'][7]['#text']\n",
    "        record_dict['platformname'] = item['str'][8]['#text']\n",
    "        record_dict['platformserialidentifier'] = item['str'][9]['#text']\n",
    "        record_dict['platformshortname'] = item['str'][10]['#text']\n",
    "        record_dict['processinglevel'] = item['str'][11]['#text']\n",
    "        record_dict['processingmode'] = item['str'][12]['#text']\n",
    "        record_dict['processingmodeabbreviation'] = item['str'][13]['#text']\n",
    "        record_dict['processorversion'] = item['str'][14]['#text']\n",
    "        record_dict['producttype'] = item['str'][15]['#text']\n",
    "        record_dict['producttypedescription'] = item['str'][16]['#text']\n",
    "        record_dict['revisionnumber'] = item['str'][17]['#text']\n",
    "        record_dict['size'] = item['str'][18]['#text']\n",
    "        record_dict['uuid'] = item['str'][19]['#text']\n",
    "        record_dict['downloadurl'] = item['link'][0]['@href']\n",
    "        print(record_dict) if silentmode==False else None\n",
    "        study_df = study_df.append(record_dict, ignore_index=True)\n",
    "        print('\\n\\n') if silentmode==False else None\n",
    "        \n",
    "    # STEP 5: Convert DataFrame to GeoDataFrame and return GeoDataFrame\n",
    "    study_df['geometry'] = study_df['footprint']\n",
    "    study_df['geometry'] = study_df['geometry'].apply(wkt.loads)\n",
    "    study_df['beginposition'] = study_df['beginposition'].replace('T',' ').astype('datetime64[ns]')\n",
    "    study_df['endposition'] = study_df['endposition'].replace('T',' ').astype('datetime64[ns]')\n",
    "    study_df['startdate'] = study_df['beginposition'].dt.strftime('%Y-%m-%d')\n",
    "    study_df['enddate'] = study_df['endposition'].dt.strftime('%Y-%m-%d')\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "    study_gdf = gpd.GeoDataFrame(study_df, crs=crs, geometry=study_df.geometry)\n",
    "    \n",
    "    return study_gdf\n",
    "\n",
    "def show_colormap():\n",
    "\n",
    "    cmaps = [('Perceptually Uniform Sequential', [\n",
    "                'viridis', 'plasma', 'inferno', 'magma', 'cividis']),\n",
    "             ('Sequential', [\n",
    "                'Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "                'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "                'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']),\n",
    "             ('Sequential (2)', [\n",
    "                'binary', 'gist_yarg', 'gist_gray', 'gray', 'bone', 'pink',\n",
    "                'spring', 'summer', 'autumn', 'winter', 'cool', 'Wistia',\n",
    "                'hot', 'afmhot', 'gist_heat', 'copper']),\n",
    "             ('Diverging', [\n",
    "                'PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu',\n",
    "                'RdYlBu', 'RdYlGn', 'Spectral', 'coolwarm', 'bwr', 'seismic']),\n",
    "             ('Cyclic', ['twilight', 'twilight_shifted', 'hsv']),\n",
    "             ('Qualitative', [\n",
    "                'Pastel1', 'Pastel2', 'Paired', 'Accent',\n",
    "                'Dark2', 'Set1', 'Set2', 'Set3',\n",
    "                'tab10', 'tab20', 'tab20b', 'tab20c']),\n",
    "             ('Miscellaneous', [\n",
    "                'flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern',\n",
    "                'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg',\n",
    "                'gist_rainbow', 'rainbow', 'jet', 'nipy_spectral', 'gist_ncar'])]\n",
    "\n",
    "\n",
    "    gradient = np.linspace(0, 1, 256)\n",
    "    gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "    for cmap_category, cmap_list in cmaps:\n",
    "        plot_color_gradients(gradient, cmap_category, cmap_list)\n",
    "\n",
    "    plt.show()\n",
    "    return True\n",
    "\n",
    "def plot_color_gradients(gradient, cmap_category, cmap_list):\n",
    "    # Create figure and adjust figure height to number of colormaps\n",
    "    nrows = len(cmap_list)\n",
    "    figh = 0.35 + 0.15 + (nrows + (nrows-1)*0.1)*0.22\n",
    "    fig, axes = plt.subplots(nrows=nrows, figsize=(6.4, figh))\n",
    "    fig.subplots_adjust(top=1-.35/figh, bottom=.15/figh, left=0.2, right=0.99)\n",
    "\n",
    "    axes[0].set_title(cmap_category + ' colormaps', fontsize=14)\n",
    "\n",
    "    for ax, name in zip(axes, cmap_list):\n",
    "        ax.imshow(gradient, aspect='auto', cmap=plt.get_cmap(name))\n",
    "        ax.text(-.01, .5, name, va='center', ha='right', fontsize=10,\n",
    "                transform=ax.transAxes)\n",
    "\n",
    "    # Turn off *all* ticks & spines, not just the ones with colormaps.\n",
    "    for ax in axes:\n",
    "        ax.set_axis_off()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
